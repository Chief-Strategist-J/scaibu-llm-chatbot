This final tree is:

* **Modular** (loose coupling)
* **Activity-based** (clear automation tasks you can implement in Temporal/Airflow)
* **Priority-graded** (P0 â†’ P3; P3 = future AI/monetization)
* **Extensible** (OTel Collector today â†’ metrics/traces later â†’ AI later)
* **Productizable** (how to package/sell as a product)

Read straight through â€” itâ€™s a single self-contained artifact you can copy into docs.

---

# START: ONE-COLLECTOR OBSERVABILITY PLATFORM (FINAL MASTER TREE)

```
START: ONE-COLLECTOR OBSERVABILITY PLATFORM
â”‚
? intent = [INGEST | QUERY | ALERT | OPERATE | CONTROL_PLANE | AI_PRODUCT]
â”‚
â”œâ”€ INGEST  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE A  (P0â†’P1)
â”œâ”€ QUERY   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE B  (P1)
â”œâ”€ ALERT   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE C  (P1â†’P2)
â”œâ”€ OPERATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE D  (P0â†’P1â†’P2)
â”œâ”€ CONTROL_PLANE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE E  (P0â†’P1â†’P2)
â””â”€ AI_PRODUCT (future ML/analytics) â”€â”€â”€â”€â”€â–º TREE F  (P3)
```

---

## TREE A â€” INGEST (Apps & Containers â†’ OpenTelemetry Collector â†’ Loki now; metrics/traces later)  **(P0 first)**

```
INGEST
â”‚
? signal_type = [logs | metrics | traces]
â”‚
â”œâ”€ LOGS (P0 - FULLY AUTOMATED)
â”‚   â”‚
â”‚   CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
â”‚   â”‚
â”‚   â”œâ”€ auto_discover_all_log_sources_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Scans Docker containers (via Docker API)
â”‚   â”‚   â€¢ Scans Kubernetes pods (via K8s API)
â”‚   â”‚   â€¢ Scans filesystem log directories (/var/log/*)
â”‚   â”‚   â€¢ Detects application-specific log paths from environment variables
â”‚   â”‚   â€¢ Returns complete inventory of all log sources
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Connect to Docker socket, subscribe to container events
â”‚   â”‚   â€¢ Watch K8s pod lifecycle events in all namespaces
â”‚   â”‚   â€¢ Recursively scan configured filesystem paths
â”‚   â”‚   â€¢ Parse container labels/pod annotations for custom log hints
â”‚   â”‚   â€¢ Merge all discovered sources into unified registry
â”‚   â”‚   â€¢ Emit "new_log_source_discovered" event for each source
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ DockerEventStreamService - maintains Docker socket connection
â”‚   â”‚   â€¢ K8sPodWatchService - maintains K8s API watch connection
â”‚   â”‚   â€¢ FilesystemScannerService - scans directories with patterns
â”‚   â”‚   â€¢ MetadataExtractorService - extracts labels/annotations/env vars
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ DiscoveredLogSource
â”‚   â”‚       - source_id: str (unique hash)
â”‚   â”‚       - source_type: enum (docker, k8s, filesystem)
â”‚   â”‚       - log_path: str
â”‚   â”‚       - metadata: Dict (container_id, pod_name, namespace, labels)
â”‚   â”‚       - discovered_at: datetime
â”‚   â”‚       - status: enum (active, inactive)
â”‚   â”‚
â”‚   â”‚   API CLIENTS:
â”‚   â”‚   â€¢ DockerAPIClient - Docker Engine API wrapper
â”‚   â”‚   â€¢ KubernetesAPIClient - K8s API wrapper
â”‚   â”‚
â”‚   â”œâ”€ auto_register_log_sources_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Takes discovered sources from previous activity
â”‚   â”‚   â€¢ Automatically registers them in LogSourceRegistry database
â”‚   â”‚   â€¢ Assigns unique IDs, timestamps, default labels
â”‚   â”‚   â€¢ No human approval needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Check if source already exists (by log_path hash)
â”‚   â”‚   â€¢ If new: insert into registry with auto-generated ID
â”‚   â”‚   â€¢ If exists: update metadata, set last_seen timestamp
â”‚   â”‚   â€¢ Assign default labels based on source type
â”‚   â”‚   â€¢ Mark inactive sources (not seen in last N minutes)
â”‚   â”‚   â€¢ Emit "log_source_registered" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ RegistryStorageService - CRUD operations on registry DB
â”‚   â”‚   â€¢ LabelGeneratorService - auto-generates default labels
â”‚   â”‚   â€¢ DeduplicationService - prevents duplicate registrations
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ RegisteredLogSource
â”‚   â”‚       - id: UUID
â”‚   â”‚       - source: DiscoveredLogSource
â”‚   â”‚       - labels: Dict[str, str]
â”‚   â”‚       - registered_at: datetime
â”‚   â”‚       - last_seen: datetime
â”‚   â”‚       - config_version: int
â”‚   â”‚
â”‚   â”œâ”€ auto_generate_collector_config_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Reads all registered sources from registry
â”‚   â”‚   â€¢ Automatically generates complete OTel Collector YAML config
â”‚   â”‚   â€¢ Includes receivers, processors, exporters for ALL sources
â”‚   â”‚   â€¢ No manual YAML editing needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Query registry for all active log sources
â”‚   â”‚   â€¢ For each source, generate filelog receiver block:
â”‚   â”‚       - include: [log_path]
â”‚   â”‚       - multiline detection (stacktraces, JSON)
â”‚   â”‚       - json_parser if JSON format detected
â”‚   â”‚       - timestamp extraction
â”‚   â”‚   â€¢ Generate resource processor with service.name, host.name
â”‚   â”‚   â€¢ Generate attributes processor for label enrichment
â”‚   â”‚   â€¢ Generate k8sattributes processor if K8s sources exist
â”‚   â”‚   â€¢ Generate batch processor (optimize throughput)
â”‚   â”‚   â€¢ Generate memory_limiter processor (prevent OOM)
â”‚   â”‚   â€¢ Generate loki exporter pointing to Loki endpoint
â”‚   â”‚   â€¢ Render complete YAML config
â”‚   â”‚   â€¢ Store config in versioned artifact store
â”‚   â”‚   â€¢ Return config_artifact_id
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ ConfigTemplateEngine - Jinja2 templates for config generation
â”‚   â”‚   â€¢ ReceiverBuilder - builds receiver configs
â”‚   â”‚   â€¢ ProcessorBuilder - builds processor configs
â”‚   â”‚   â€¢ ExporterBuilder - builds exporter configs
â”‚   â”‚   â€¢ ConfigValidator - validates generated YAML
â”‚   â”‚   â€¢ ConfigArtifactStore - stores versioned configs
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ CollectorConfig
â”‚   â”‚       - receivers: Dict[str, Any]
â”‚   â”‚       - processors: Dict[str, Any]
â”‚   â”‚       - exporters: Dict[str, Any]
â”‚   â”‚       - service_pipelines: Dict[str, Pipeline]
â”‚   â”‚       - version: int
â”‚   â”‚       - generated_at: datetime
â”‚   â”‚
â”‚   â”œâ”€ auto_deploy_collector_config_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Takes generated config from previous activity
â”‚   â”‚   â€¢ Automatically deploys it to OTel Collector instance
â”‚   â”‚   â€¢ Triggers hot reload (no collector restart)
â”‚   â”‚   â€¢ No manual kubectl/docker commands needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Fetch config artifact from store
â”‚   â”‚   â€¢ Push config to collector's config endpoint (HTTP API)
â”‚   â”‚   â€¢ OR write config to mounted volume if collector watches file
â”‚   â”‚   â€¢ Trigger collector reload via reload endpoint
â”‚   â”‚   â€¢ Wait for reload acknowledgment
â”‚   â”‚   â€¢ Poll collector health endpoint
â”‚   â”‚   â€¢ Emit "config_deployed" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ CollectorAPIClient - HTTP client for collector management API
â”‚   â”‚   â€¢ ConfigDeploymentService - handles config push & reload
â”‚   â”‚   â€¢ HealthCheckService - polls collector health
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ DeploymentResult
â”‚   â”‚       - config_version: int
â”‚   â”‚       - deployed_at: datetime
â”‚   â”‚       - collector_id: str
â”‚   â”‚       - status: enum (success, failed)
â”‚   â”‚       - rollback_config: Optional[int]
â”‚   â”‚
â”‚   â”œâ”€ auto_verify_log_pipeline_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically tests that logs are flowing end-to-end
â”‚   â”‚   â€¢ Injects test log entry
â”‚   â”‚   â€¢ Queries Loki to verify it arrived
â”‚   â”‚   â€¢ No manual verification needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Generate unique test log message with UUID
â”‚   â”‚   â€¢ Write test log to one of the monitored log files
â”‚   â”‚   â€¢ Wait 30 seconds for ingestion
â”‚   â”‚   â€¢ Query Loki for test log using UUID
â”‚   â”‚   â€¢ Check if log found with correct labels
â”‚   â”‚   â€¢ Return verification result
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ TestLogInjectorService - writes test logs
â”‚   â”‚   â€¢ LokiQueryService - queries Loki API
â”‚   â”‚   â€¢ VerificationService - compares expected vs actual
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ VerificationResult
â”‚   â”‚       - test_id: UUID
â”‚   â”‚       - log_found: bool
â”‚   â”‚       - latency_seconds: float
â”‚   â”‚       - labels_correct: bool
â”‚   â”‚       - verified_at: datetime
â”‚   â”‚
â”‚   â””â”€ auto_rollback_on_failure_activity
â”‚       WHAT IT DOES:
â”‚       â€¢ If verification fails, automatically rolls back config
â”‚       â€¢ Restores previous working config version
â”‚       â€¢ Sends alert to monitoring
â”‚       â€¢ No manual intervention needed
â”‚
â”‚       LOGIC:
â”‚       â€¢ Detect failed verification from previous activity
â”‚       â€¢ Fetch last known good config version from artifact store
â”‚       â€¢ Deploy previous config to collector
â”‚       â€¢ Trigger reload
â”‚       â€¢ Verify rollback succeeded
â”‚       â€¢ Send alert with failure details
â”‚       â€¢ Mark failed config as "broken"
â”‚
â”‚       SERVICES NEEDED:
â”‚       â€¢ RollbackService - manages config version history
â”‚       â€¢ AlertingService - sends alerts to Slack/PagerDuty
â”‚       â€¢ ConfigDeploymentService (reuse)
â”‚
â”‚       MODELS:
â”‚       â€¢ RollbackEvent
â”‚           - failed_config_version: int
â”‚           - rollback_to_version: int
â”‚           - reason: str
â”‚           - rolled_back_at: datetime
â”‚
â”œâ”€ METRICS (P1 - FULLY AUTOMATED)
â”‚   â”‚
â”‚   CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
â”‚   â”‚
â”‚   â”œâ”€ auto_discover_all_metric_sources_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Scans K8s pods for prometheus.io/scrape=true annotations
â”‚   â”‚   â€¢ Detects OTLP-enabled applications (via env vars or labels)
â”‚   â”‚   â€¢ Discovers host metrics endpoints
â”‚   â”‚   â€¢ Returns complete inventory of metric sources
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Query K8s API for all pods in all namespaces
â”‚   â”‚   â€¢ Filter pods with prometheus.io/scrape="true"
â”‚   â”‚   â€¢ Extract prometheus.io/port and prometheus.io/path
â”‚   â”‚   â€¢ Scan pods for OTEL_EXPORTER_OTLP_ENDPOINT env var
â”‚   â”‚   â€¢ Detect applications with OTLP SDK libraries loaded
â”‚   â”‚   â€¢ Identify host metric collection requirement (node-level)
â”‚   â”‚   â€¢ Emit "new_metric_source_discovered" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ K8sAnnotationScanner - reads pod annotations
â”‚   â”‚   â€¢ OTLPEndpointDetector - detects OTLP-enabled apps
â”‚   â”‚   â€¢ HostMetricsDetector - identifies nodes needing host metrics
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ DiscoveredMetricSource
â”‚   â”‚       - source_id: str
â”‚   â”‚       - source_type: enum (prometheus_scrape, otlp, hostmetrics)
â”‚   â”‚       - endpoint: str
â”‚   â”‚       - scrape_config: Optional[ScrapeConfig]
â”‚   â”‚       - metadata: Dict
â”‚   â”‚       - discovered_at: datetime
â”‚   â”‚
â”‚   â”œâ”€ auto_register_metric_sources_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically registers discovered metric sources
â”‚   â”‚   â€¢ Assigns IDs, default labels, scrape intervals
â”‚   â”‚   â€¢ No human approval needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Check if metric source already exists
â”‚   â”‚   â€¢ Insert new sources into MetricSourceRegistry
â”‚   â”‚   â€¢ Update existing sources with latest metadata
â”‚   â”‚   â€¢ Assign default scrape_interval based on source type
â”‚   â”‚   â€¢ Emit "metric_source_registered" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ MetricRegistryService - CRUD for metric sources
â”‚   â”‚   â€¢ ScrapeConfigGenerator - generates scrape configs
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ RegisteredMetricSource
â”‚   â”‚       - id: UUID
â”‚   â”‚       - source: DiscoveredMetricSource
â”‚   â”‚       - scrape_interval: int
â”‚   â”‚       - labels: Dict[str, str]
â”‚   â”‚       - registered_at: datetime
â”‚   â”‚
â”‚   â”œâ”€ auto_generate_metric_pipeline_config_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically generates OTel Collector metric pipeline config
â”‚   â”‚   â€¢ Adds OTLP receiver, Prometheus scrape receiver, hostmetrics receiver
â”‚   â”‚   â€¢ Configures Prometheus remote_write exporter
â”‚   â”‚   â€¢ No manual YAML editing needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Query MetricSourceRegistry for all active sources
â”‚   â”‚   â€¢ Generate OTLP receiver (gRPC + HTTP) if OTLP sources exist
â”‚   â”‚   â€¢ Generate Prometheus receiver with scrape_configs for Prometheus sources
â”‚   â”‚   â€¢ Generate hostmetrics receiver if host monitoring needed
â”‚   â”‚   â€¢ Generate batch, memory_limiter, resource processors
â”‚   â”‚   â€¢ Generate prometheusremotewrite exporter to Prometheus endpoint
â”‚   â”‚   â€¢ Merge with existing log pipeline config
â”‚   â”‚   â€¢ Validate complete config
â”‚   â”‚   â€¢ Store versioned config artifact
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ MetricPipelineBuilder - builds metric pipeline config
â”‚   â”‚   â€¢ ConfigMerger - merges log + metric pipelines
â”‚   â”‚   â€¢ ConfigValidator (reuse)
â”‚   â”‚   â€¢ ConfigArtifactStore (reuse)
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ MetricPipelineConfig
â”‚   â”‚       - receivers: Dict[str, Any]
â”‚   â”‚       - processors: Dict[str, Any]
â”‚   â”‚       - exporters: Dict[str, Any]
â”‚   â”‚       - version: int
â”‚   â”‚
â”‚   â”œâ”€ auto_deploy_metric_pipeline_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically deploys updated config to collector
â”‚   â”‚   â€¢ Hot reloads collector with metric pipeline enabled
â”‚   â”‚   â€¢ No manual deployment needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ (Same as auto_deploy_collector_config_activity)
â”‚   â”‚   â€¢ Push merged config to collector
â”‚   â”‚   â€¢ Trigger reload
â”‚   â”‚   â€¢ Wait for health check
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ CollectorAPIClient (reuse)
â”‚   â”‚   â€¢ ConfigDeploymentService (reuse)
â”‚   â”‚
â”‚   â”œâ”€ auto_verify_metric_pipeline_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically verifies metrics are flowing to Prometheus
â”‚   â”‚   â€¢ Queries Prometheus for expected metrics
â”‚   â”‚   â€¢ No manual verification needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Wait 60 seconds for metrics to arrive
â”‚   â”‚   â€¢ Query Prometheus for known metric names from discovered sources
â”‚   â”‚   â€¢ Verify metric labels match expected values
â”‚   â”‚   â€¢ Check scrape success rate
â”‚   â”‚   â€¢ Return verification result
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ PrometheusQueryService - queries Prometheus API
â”‚   â”‚   â€¢ MetricVerificationService - validates metrics
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ MetricVerificationResult
â”‚   â”‚       - metrics_found: int
â”‚   â”‚       - scrape_success_rate: float
â”‚   â”‚       - verified_at: datetime
â”‚   â”‚
â”‚   â””â”€ auto_rollback_on_metric_failure_activity
â”‚       WHAT IT DOES:
â”‚       â€¢ Automatically rolls back if metric pipeline fails
â”‚       â€¢ (Same logic as auto_rollback_on_failure_activity)
â”‚
â””â”€ TRACES (P1 - FULLY AUTOMATED)
    â”‚
    CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
    â”‚
    â”œâ”€ auto_discover_all_trace_sources_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Detects applications sending OTLP traces
    â”‚   â€¢ Detects Jaeger-instrumented applications
    â”‚   â€¢ Detects Zipkin-instrumented applications
    â”‚   â€¢ Returns complete inventory of trace sources
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ Scan pods for OTEL_TRACES_EXPORTER env var
    â”‚   â€¢ Detect Jaeger client libraries in running processes
    â”‚   â€¢ Detect Zipkin client libraries in running processes
    â”‚   â€¢ Check for trace endpoints being accessed (network traffic analysis)
    â”‚   â€¢ Emit "new_trace_source_discovered" event
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ TraceEndpointDetector - detects trace instrumentation
    â”‚   â€¢ NetworkTrafficAnalyzer - monitors trace endpoint traffic
    â”‚
    â”‚   MODELS:
    â”‚   â€¢ DiscoveredTraceSource
    â”‚       - source_id: str
    â”‚       - source_type: enum (otlp, jaeger, zipkin)
    â”‚       - protocol: str
    â”‚       - endpoint: str
    â”‚       - metadata: Dict
    â”‚
    â”œâ”€ auto_register_trace_sources_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically registers discovered trace sources
    â”‚   â€¢ No human approval needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ Insert trace sources into TraceSourceRegistry
    â”‚   â€¢ Assign default sampling rates
    â”‚   â€¢ Emit "trace_source_registered" event
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ TraceRegistryService - CRUD for trace sources
    â”‚
    â”‚   MODELS:
    â”‚   â€¢ RegisteredTraceSource
    â”‚       - id: UUID
    â”‚       - source: DiscoveredTraceSource
    â”‚       - sampling_rate: float
    â”‚       - registered_at: datetime
    â”‚
    â”œâ”€ auto_generate_trace_pipeline_config_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically generates OTel Collector trace pipeline config
    â”‚   â€¢ Adds OTLP, Jaeger, Zipkin receivers as needed
    â”‚   â€¢ Configures Tempo and/or Jaeger exporters
    â”‚   â€¢ No manual YAML editing needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ Query TraceSourceRegistry for all active sources
    â”‚   â€¢ Generate OTLP receiver if OTLP sources exist
    â”‚   â€¢ Generate Jaeger receiver if Jaeger sources exist
    â”‚   â€¢ Generate Zipkin receiver if Zipkin sources exist
    â”‚   â€¢ Generate trace processors (batch, sampling, memory_limiter)
    â”‚   â€¢ Generate Tempo exporter (OTLP to Tempo)
    â”‚   â€¢ Generate Jaeger exporter if Jaeger backend configured
    â”‚   â€¢ Merge with existing log + metric pipeline config
    â”‚   â€¢ Validate complete config
    â”‚   â€¢ Store versioned config artifact
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ TracePipelineBuilder - builds trace pipeline config
    â”‚   â€¢ ConfigMerger (reuse)
    â”‚   â€¢ ConfigValidator (reuse)
    â”‚   â€¢ ConfigArtifactStore (reuse)
    â”‚
    â”‚   MODELS:
    â”‚   â€¢ TracePipelineConfig
    â”‚       - receivers: Dict[str, Any]
    â”‚       - processors: Dict[str, Any]
    â”‚       - exporters: Dict[str, Any]
    â”‚       - version: int
    â”‚
    â”œâ”€ auto_deploy_trace_pipeline_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically deploys updated config to collector
    â”‚   â€¢ Hot reloads collector with trace pipeline enabled
    â”‚   â€¢ No manual deployment needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ (Same as auto_deploy_collector_config_activity)
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ CollectorAPIClient (reuse)
    â”‚   â€¢ ConfigDeploymentService (reuse)
    â”‚
    â”œâ”€ auto_verify_trace_pipeline_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically verifies traces are flowing to Tempo/Jaeger
    â”‚   â€¢ Generates test trace and queries backend
    â”‚   â€¢ No manual verification needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ Inject test trace with unique trace_id
    â”‚   â€¢ Wait 60 seconds for trace to arrive
    â”‚   â€¢ Query Tempo/Jaeger for test trace_id
    â”‚   â€¢ Verify trace found with correct spans
    â”‚   â€¢ Return verification result
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ TestTraceGenerator - generates synthetic traces
    â”‚   â€¢ TempoQueryService - queries Tempo API
    â”‚   â€¢ JaegerQueryService - queries Jaeger API
    â”‚
    â”‚   MODELS:
    â”‚   â€¢ TraceVerificationResult
    â”‚       - trace_found: bool
    â”‚       - span_count: int
    â”‚       - verified_at: datetime
    â”‚
    â””â”€ auto_rollback_on_trace_failure_activity
        WHAT IT DOES:
        â€¢ Automatically rolls back if trace pipeline fails
        â€¢ (Same logic as auto_rollback_on_failure_activity)
```

**INGEST: Key design rules**

* Keep labels low-cardinality (enforce in `attributes_processor`).
* All activities idempotent & time-bounded.
* Use event bus (Kafka/NATS) for discovery events to decouple watchers from builder.
* Persist registry in transactional DB (Postgres/etcd) with schema versioning.

---

## TREE B â€” QUERY (Grafana + API; Logs now, Metrics/Traces later)  **(P1)**

```
QUERY
â”‚
? signal = [logs | metrics | traces]
â”‚
â”œâ”€ LOGS (P1 - FULLY AUTOMATED)
â”‚   â”‚
â”‚   CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
â”‚   â”‚
â”‚   â”œâ”€ auto_create_loki_datasource_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically creates Loki datasource in Grafana
â”‚   â”‚   â€¢ Detects Loki endpoint from service discovery
â”‚   â”‚   â€¢ Configures auth automatically
â”‚   â”‚   â€¢ No manual Grafana UI clicks needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Discover Loki service endpoint (K8s service DNS or Docker network)
â”‚   â”‚   â€¢ Check if Loki datasource already exists in Grafana
â”‚   â”‚   â€¢ If not exists: create via Grafana API
â”‚   â”‚   â€¢ Configure derived fields for trace linking (extract trace_id from logs)
â”‚   â”‚   â€¢ Set as default datasource for logs
â”‚   â”‚   â€¢ Test datasource connection
â”‚   â”‚   â€¢ Emit "loki_datasource_created" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ ServiceDiscoveryClient - discovers Loki endpoint
â”‚   â”‚   â€¢ GrafanaAPIClient - manages Grafana datasources
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ GrafanaDatasource
â”‚   â”‚       - name: str
â”‚   â”‚       - type: str
â”‚   â”‚       - url: str
â”‚   â”‚       - is_default: bool
â”‚   â”‚       - derived_fields: List[DerivedField]
â”‚   â”‚
â”‚   â”œâ”€ auto_create_log_dashboards_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically generates Grafana dashboards for logs
â”‚   â”‚   â€¢ Creates dashboards per service, namespace, pod
â”‚   â”‚   â€¢ No manual dashboard creation needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Query LogSourceRegistry for all registered services
â”‚   â”‚   â€¢ For each service, generate dashboard JSON with:
â”‚   â”‚       - Log volume panel (rate of logs per service)
â”‚   â”‚       - Error rate panel (count of error-level logs)
â”‚   â”‚       - Log stream panel (live tail)
â”‚   â”‚       - Log level distribution panel
â”‚   â”‚   â€¢ Push dashboard to Grafana via API
â”‚   â”‚   â€¢ Create folder structure (e.g., "Auto-Generated Logs")
â”‚   â”‚   â€¢ Emit "dashboard_created" event
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ DashboardTemplateEngine - generates dashboard JSON
â”‚   â”‚   â€¢ GrafanaAPIClient (reuse)
â”‚   â”‚
â”‚   â”‚   MODELS:
â”‚   â”‚   â€¢ GrafanaDashboard
â”‚   â”‚       - title: str
â”‚   â”‚       - panels: List[Panel]
â”‚   â”‚       - folder: str
â”‚   â”‚       - uid: str
â”‚   â”‚
â”‚   â””â”€ auto_configure_log_query_caching_activity
â”‚       WHAT IT DOES:
â”‚       â€¢ Automatically configures query frontend caching
â”‚       â€¢ No manual Redis/Memcached setup needed
â”‚
â”‚       LOGIC:
â”‚       â€¢ Deploy Redis/Memcached if not exists
â”‚       â€¢ Configure Loki query-frontend to use cache
â”‚       â€¢ Set cache TTL based on query patterns
â”‚       â€¢ Emit "caching_configured" event
â”‚
â”‚       SERVICES NEEDED:
â”‚       â€¢ CacheDeploymentService - deploys cache backend
â”‚       â€¢ LokiConfigService - updates Loki config
â”‚
â”œâ”€ METRICS (P1 - FULLY AUTOMATED)
â”‚   â”‚
â”‚   CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
â”‚   â”‚
â”‚   â”œâ”€ auto_create_prometheus_datasource_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically creates Prometheus datasource in Grafana
â”‚   â”‚   â€¢ No manual configuration needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Discover Prometheus endpoint
â”‚   â”‚   â€¢ Check if datasource exists
â”‚   â”‚   â€¢ Create datasource via Grafana API
â”‚   â”‚   â€¢ Set as default for metrics
â”‚   â”‚   â€¢ Test connection
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ ServiceDiscoveryClient (reuse)
â”‚   â”‚   â€¢ GrafanaAPIClient (reuse)
â”‚   â”‚
â”‚   â”œâ”€ auto_create_metric_dashboards_activity
â”‚   â”‚   WHAT IT DOES:
â”‚   â”‚   â€¢ Automatically generates Grafana dashboards for metrics
â”‚   â”‚   â€¢ Creates RED dashboards (Rate, Errors, Duration) per service
â”‚   â”‚   â€¢ No manual dashboard creation needed
â”‚   â”‚
â”‚   â”‚   LOGIC:
â”‚   â”‚   â€¢ Query MetricSourceRegistry for all services
â”‚   â”‚   â€¢ For each service, generate dashboard with:
â”‚   â”‚       - Request rate panel
â”‚   â”‚       - Error rate panel
â”‚   â”‚       - Request duration (latency) panel
â”‚   â”‚       - CPU/Memory usage panels
â”‚   â”‚       - Saturation metrics
â”‚   â”‚   â€¢ Push dashboards to Grafana
â”‚   â”‚
â”‚   â”‚   SERVICES NEEDED:
â”‚   â”‚   â€¢ DashboardTemplateEngine (reuse)
â”‚   â”‚   â€¢ GrafanaAPIClient (reuse)
â”‚   â”‚
â”‚   â””â”€ auto_create_slo_dashboards_activity
â”‚       WHAT IT DOES:
â”‚       â€¢ Automatically creates SLO tracking dashboards
â”‚       â€¢ No manual SLO definition needed (uses defaults)
â”‚
â”‚       LOGIC:
â”‚       â€¢ Define default SLOs (99.9% availability, p99 < 500ms)
â”‚       â€¢ Generate SLO dashboard with error budget burn rate
â”‚       â€¢ Push to Grafana
â”‚
â”‚       SERVICES NEEDED:
â”‚       â€¢ SLODashboardGenerator
â”‚       â€¢ GrafanaAPIClient (reuse)
â”‚
â””â”€ TRACES (P1 - FULLY AUTOMATED)
    â”‚
    CONTROL PLANE TRIGGERS THESE ACTIVITIES AUTOMATICALLY:
    â”‚
    â”œâ”€ auto_create_tempo_datasource_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically creates Tempo datasource in Grafana
    â”‚   â€¢ No manual configuration needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ Discover Tempo endpoint
    â”‚   â€¢ Check if datasource exists
    â”‚   â€¢ Create datasource via Grafana API
    â”‚   â€¢ Configure trace-to-logs linking (Loki derived fields)
    â”‚   â€¢ Test connection
    â”‚
    â”‚   SERVICES NEEDED:
    â”‚   â€¢ ServiceDiscoveryClient (reuse)
    â”‚   â€¢ GrafanaAPIClient (reuse)
    â”‚
    â”œâ”€ auto_create_jaeger_datasource_activity
    â”‚   WHAT IT DOES:
    â”‚   â€¢ Automatically creates Jaeger datasource if Jaeger backend used
    â”‚   â€¢ No manual configuration needed
    â”‚
    â”‚   LOGIC:
    â”‚   â€¢ (Same as Tempo datasource creation)
    â”‚
    â””â”€ auto_create_trace_dashboards_activity
        WHAT IT DOES:
        â€¢ Automatically generates service dependency map dashboard
        â€¢ Creates trace latency dashboards per service
        â€¢ No manual dashboard creation needed
    
        LOGIC:
        â€¢ Query TraceSourceRegistry for all services
        â€¢ Generate service map dashboard (nodes = services, edges = calls)
        â€¢ Generate latency distribution dashboard per service
        â€¢ Push dashboards to Grafana
    
        SERVICES NEEDED:
        â€¢ TraceDashboardGenerator
        â€¢ GrafanaAPIClient (reuse)
```

**QUERY: Notes**

* Query Frontend caching + rate limiting to protect backends.
* BatchQueryExecutor splits large time windows into fan-out jobs.

---

## TREE C â€” ALERT (Log-based & metric-based alerts)  **(P1)**

```
ALERT
â”‚
? engine = [loki_ruler | grafana_alerts | prom_alerts]
â”‚
â”œâ”€ loki_ruler (log rules)
â”‚   CLASSES: LokiRulerClient
â”‚   METHODS: create_rule(), test_rule()
â”‚   ACTIVITIES:
â”‚     - deploy_log_rule_activity
â”‚     - evaluate_and_fire_activity
â”‚
â”œâ”€ grafana_alerts
â”‚   ACTIVITIES:
â”‚     - create_grafana_alert_activity
â”‚     - route_alert_activity
â”‚
â””â”€ prom_alerts (later, P1)
    ACTIVITIES:
      - create_prometheus_rule_activity
```

**ALERT: Design**

* Use dedupe & grouping in AlertGenerator.
* Include runbooks links in alerts.
* Tempo linking for trace on alert (P2).

---

## TREE D â€” OPERATE (Lifecycle, Scale, Security, SLOs)  **(P0â†’P1â†’P2)**

```
OPERATE
â”‚
? maturity = [P0 | P1 | P2]
â”‚
â”œâ”€ P0: Lifecycle (essential)
â”‚   ACTIVITIES:
â”‚     - start_collector_activity
â”‚     - stop_collector_activity
â”‚     - reload_collector_activity
â”‚     - start_loki_activity
â”‚     - verify_services_health_activity
â”‚   ACCEPTANCE:
â”‚     - collector / loki ready probes pass
â”‚
â”œâ”€ P1: Scale & Storage
â”‚   CLASSES:
â”‚     - ChunkStorageBackend (S3/MinIO)
â”‚     - HealthMonitor
â”‚   ACTIVITIES:
â”‚     - scale_component_activity(component, replicas)
â”‚     - verify_replication_activity()
â”‚     - compact_and_retention_activity()
â”‚   ACCEPTANCE:
â”‚     - chunk upload success rate > 99.9%
â”‚
â””â”€ P2: Security, Tenancy & Governance
    CLASSES:
      - CertManager
      - GatewayAuthMiddleware
      - AccessPolicyManager
      - SLOEvaluator
    ACTIVITIES:
      - issue_agent_certificate_activity()
      - rotate_certificate_activity()
      - enforce_tenant_quota_activity()
      - evaluate_SLOs_activity()
      - run_chaos_test_activity()
    ACCEPTANCE:
      - mTLS validated; per-tenant quotas in effect; SLO alerts configured
```

**OPERATE: Principles**

* Automation for lifecycle via Temporal / Kubernetes operators.
* All lifecycle activities idempotent & safe to re-run.
* Extensive metrics & dashboards for platform health.

---

## TREE E â€” CONTROL_PLANE (Discovery â†’ Registry â†’ Config Builder â†’ AgentManager)  **(P0â†’P2)**

```
CONTROL_PLANE
â”‚
? mode = [simple_polling | event_stream (recommended) | hybrid]
â”‚
â”œâ”€ DiscoveryService
â”‚   METHODS:
â”‚     - watch_docker_events()
â”‚     - watch_k8s_api()
â”‚     - watch_filesystem()
â”‚     - emit_discovery_event()
â”‚   ACTIVITIES:
â”‚     - docker_watch_activity
â”‚     - k8s_watch_activity
â”‚
â”œâ”€ LogSourceRegistry
â”‚   METHODS:
â”‚     - register_source(metadata)
â”‚     - update_source()
â”‚     - delete_source()
â”‚     - list_sources()
â”‚   ACTIVITIES:
â”‚     - upsert_registry_record_activity
â”‚
â”œâ”€ OtelConfigBuilder
â”‚   METHODS:
â”‚     - load_registry_state()
â”‚     - generate_log_pipeline()
â”‚     - generate_metrics_pipeline()
â”‚     - generate_traces_pipeline()
â”‚     - validate_config()
â”‚     - render_yaml()
â”‚   ACTIVITIES:
â”‚     - build_config_activity
â”‚     - validate_config_activity
â”‚
â”œâ”€ AgentManager
â”‚   METHODS:
â”‚     - push_config_to_collector()
â”‚     - trigger_reload_api()
â”‚     - check_reload_result()
â”‚   ACTIVITIES:
â”‚     - push_config_activity
â”‚     - canary_reload_activity (P1)
â”‚     - rollback_config_activity
â”‚
â””â”€ Coordination (workflow)
    ACTIVITIES:
      - promtail_dynamic_reload_workflow (rename â†’ collector_dynamic_reload_workflow)
      - schedule_reconcile_activity
      - handle_config_change_event_activity
```

**CONTROL_PLANE: Architecture**

* **Event-driven**: discovery emits events â†’ config builder subscribes â†’ generates config â†’ agentmanager reloads.
* **Message bus** recommended (Kafka, NATS) for loose coupling & replayability.
* **Config versioning** + immutable artifacts + rollback.

---

## TREE F â€” AI_PRODUCT (Analytics, ML, Monetization)  **(P3 future)**

```
AI_PRODUCT
â”‚
? goal = [smart_routing | anomaly_detection | insights_market]
â”‚
â”œâ”€ Anomaly Detection & Insights (P3)
â”‚   USE:
â”‚     â€¢ Training Data Pipeline
â”‚         - export_labelled_logs_activity
â”‚         - transform_and_enrich_activity
â”‚         - store_training_artifact_activity
â”‚     â€¢ Model Training & Serving
â”‚         - train_model_activity
â”‚         - evaluate_model_activity
â”‚         - serve_model_inference_activity
â”‚     â€¢ Real-time Detection
â”‚         - stream_inference_activity
â”‚         - raise_ai_alert_activity
â”‚
â”œâ”€ Smart Routing & Auto-Remediation (P3)
â”‚   ACTIVITIES:
â”‚     - recommend_relabel_rules_activity
â”‚     - auto_rollout_config_activity (human-in-loop)
â”‚
â””â”€ Productization & Marketplace (P3)
    ACTIVITIES:
      - package_saas_offer_activity
      - provide_self_hosted_helm_chart_activity
      - telemetry_privacy_and_compliance_activity
```

**AI_PRODUCT: Design**

* Strict data governance & opt-in for training data.
* Use offline exports + synthetic labels for supervised models.
* Human-in-loop for auto actions (canary + approval).
* Monetize via features (anomaly detection, insights, managed hosting).

---

# MODULES / CLASSES SUMMARY (Single-screen)

```
CONTROL PLANE:
  DiscoveryService, LogSourceRegistry, RegistryRepository,
  OtelConfigBuilder, ConfigTemplateLoader, AgentManager, HealthMonitor

DATA PLANE:
  OTelCollectorAgent, LokiDistributorClient, LokiQuerierClient, LokiRulerClient, ChunkStorageBackend

SDK (minimal; optional):
  Auto-instrumentation wrappers (startup scripts only)

SECURITY:
  CertManager, GatewayAuthMiddleware, AccessPolicyManager

OBSERVABILITY:
  MetricsCollector, SLOEvaluator, AlertGenerator, ChaosTester

AI/MODELS:
  TrainingPipeline, ModelTrainer, InferenceService, DataExportService
```

---

# ACTIVITIES (Actionable, Activity-level names for Temporal / Orchestration)

> Activities are intentionally fine-grained and **loosely coupled** so you can compose them into workflows.

**Discovery / Registry / Config**

```
docker_watch_activity
k8s_watch_activity
filesystem_watch_activity
register_service_activity
deregister_service_activity
upsert_registry_record_activity
build_config_activity
validate_config_activity
render_config_activity
store_config_artifact_activity
```

**Deployment / Agent**

```
push_config_activity
canary_reload_activity
rollback_config_activity
verify_reload_activity
verify_collector_health_activity
list_active_collectors_activity
register_new_collector_activity
```

**Ingestion / Shipping**

```
discover_log_files_activity
tail_and_ship_logs_activity
collect_otlp_activity
route_streams_activity
label_enrichment_activity
```

**Query / Export / Alerts**

```
serve_query_activity
run_batch_query_activity
export_logs_to_datalake_activity
deploy_log_rule_activity
evaluate_rule_activity
send_alert_activity
```

**Ops / Scale / Security**

```
start_collector_activity
stop_collector_activity
restart_collector_activity
start_loki_activity
scale_component_activity
issue_agent_certificate_activity
rotate_certificate_activity
enforce_tenant_quota_activity
evaluate_SLOs_activity
run_chaos_test_activity
```

**AI / ML (P3)**

```
export_labelled_logs_activity
preprocess_training_data_activity
train_model_activity
evaluate_model_activity
deploy_model_activity
stream_inference_activity
recommend_config_activity
auto_rollout_recommendation_activity
```

---

# FULL WORKFLOW (Event-driven sequence â€” ASCII)

```
[Container start] â”€â”€â–¶ docker_k8s_watch_activity â”€â”€â–¶ register_service_activity â”€â”€â–¶ upsert_registry_record_activity
     â”‚                                                                                     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ build_config_activity â”€â”€â–¶ validate_config_activity â”€â”€â–¶ store_config_artifact_activity
                                                                                                 â”‚
                                                                                                 â””â”€â–¶ push_config_activity â”€â”€â–¶ canary_reload_activity â”€â”€â–¶ verify_reload_activity
                                                                                                                              â”‚
                                                                                                                              â”œâ”€ if ok â”€â–¶ promote_config_activity
                                                                                                                              â””â”€ if fail â”€â–¶ rollback_config_activity + send_alert_activity
```

---

# PRIORITY ROADMAP (Concrete, with deliverables & acceptance tests)

**P0 (Weeks 0â€“2) â€” Logs working end-to-end**

* Deliverables:

  * Loki + Grafana (dev compose)
  * OTel Collector (filelogâ†’Loki)
  * DiscoveryService basic (docker watcher)
  * LogSourceRegistry (sqlite)
  * OtelConfigBuilder (logs only)
  * AgentManager push & reload
  * Temporal workflow `collector_dynamic_reload_workflow`
* Acceptance:

  * New container â†’ logs appear in Grafana within 30s
  * Collector / Loki health checks green

**P1 (Weeks 3â€“8) â€” Hardening & Metrics**

* Deliverables:

  * Move Loki to distributed + object store
  * Query Frontend + caching
  * LokiRuler alerts + dashboards
  * Add OTLP receiver + prometheusremotewrite pipeline
  * Auto-instrumentation wrappers & docs
  * Canary reload & rollback support
* Acceptance:

  * Metrics available in Grafana; alerts fire on synthetic tests
  * Canary reload validated, rollback works automatically

**P2 (Weeks 9â€“16) â€” Security, multi-tenant & SLOs**

* Deliverables:

  * mTLS, CertManager, per-tenant OrgID
  * Quotas & retention policies (AccessPolicyManager)
  * SLOEvaluator + Alerting pipeline
  * Chaos tests & runbooks
* Acceptance:

  * mTLS validated; tenant isolation confirmed; SLOs enforced

**P3 (Months 4â€“ongoing) â€” AI/Monetization**

* Deliverables:

  * Training data export pipeline
  * ML models (anomaly detection & recommendations)
  * Human-in-loop auto-remediation features
  * Packaging: SaaS offering + Helm chart + docs + billing hooks
* Acceptance:

  * Models trained on representative data, anomaly alerts reduce MTTD
  * Productized offering (pricing, onboarding, SLAs)

---

# DESIGN PRINCIPLES (to enforce during build)

1. **Loose coupling** â€” use event bus + activities; no synchronous blocking between discovery and builder.
2. **Idempotency** â€” all activities are retryable and idempotent.
3. **Config as Artifact** â€” version configs, store immutable artifacts, allow rollback.
4. **Human-in-loop** â€” canary + approval before global rollouts (especially for AI actions).
5. **Privacy & Compliance** â€” redact PII in collectors; opt-in for training exports.
6. **Observability of the observability** â€” metrics for every component + alerts.
7. **Product-ready modularity** â€” small microservices or functions for each activity so you can license/sell components.

---

# âœ… HIGH-LEVEL FLOW (Plain-English)

```
A service runs â†’ It produces logs â†’ Logs written to container stdout
â†’ OpenTelemetry Collector reads them â†’ Normalizes & labels them
â†’ Sends them to Loki â†’ Visible in Grafana

Meanwhileâ€¦

Control Plane watches for new/removed services
â†’ Updates registry â†’ Rebuilds collector config â†’ Reloads collector safely
â†’ Verifies pipeline health â†’ Alerts if issues â†’ Auto-recovers if needed
```

---

# âœ… SYSTEM FLOW (ASCII SEQUENCE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Pods â”‚   (no instrumentation needed)
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ stdout logs
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Container Log Files     â”‚  (/var/log/containers/*.log)
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ filelog receiver reads files
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenTelemetry Collector Agent    â”‚ (Single Collector)
â”‚   filelog_receiver()             â”‚
â”‚   + attributes/resource/batch     â”‚
â”‚   + loki_exporter()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ pushes logs
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Loki       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ query API
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Grafana     â”‚ (Explore/Alerts)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# âœ… CONTROL PLANE FLOW (DYNAMIC AUTO-DISCOVERY + CONFIG ROLLOUT)

```
       [New Service Starts]
                   â”‚
                   â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ DiscoveryService       â”‚ (watches docker/k8s events)
      â”‚ detect_service()       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ emits event
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ LogSourceRegistry      â”‚
      â”‚ register_log_target()  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ fetch registry state
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ OtelConfigBuilder      â”‚
      â”‚ generate_config()      â”‚ (log pipeline only in P0)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ push config
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ AgentManager           â”‚
      â”‚ reload_collector()     â”‚ (hot reload, no downtime)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ verify
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ HealthMonitor          â”‚
      â”‚ verify_pipeline_ok()   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      if fail:     â”‚
      rollback, alert, retry
```

---

# âœ… TEMPORAL WORKFLOW LOOP (Automation)

```
LOOP forever (every 15-30s or event-triggered)
â”‚
â”œâ”€ docker_watch_activity
â”œâ”€ k8s_watch_activity
â”‚
â”œâ”€ register_or_update_registry_activity
â”‚
â”œâ”€ build_config_activity
â”œâ”€ validate_config_activity
â”‚
â”œâ”€ push_config_activity
â”œâ”€ canary_reload_activity
â”‚
â”œâ”€ verify_reload_activity
â”‚
â””â”€ if failed â†’ rollback_config_activity + send_alert_activity
```

This workflow **never stops** â€” it is the *brain* of the system.

---

# âœ… FUTURE FLOW (WHEN YOU LATER ADD METRICS + TRACES)

You **do not** change the system, you **only add 2 lines in config**:

```
APP â†’ otel auto-instrument â†’ otlp â†’ Collector
                   â”‚
                   â”œâ”€ metrics â†’ Prometheus
                   â””â”€ traces â†’ Tempo/Jaeger
```

The **Control Plane** already knows how to:

```
generate_metrics_pipeline()
generate_traces_pipeline()
reload_collector()
verify_health()
```

So **the same workflow continues working**.

---

# âœ… WHAT MAKES THIS DESIGN CORRECT & FUTURE-PROOF

| Goal                      | Achieved By                                    |
| ------------------------- | ---------------------------------------------- |
| No code changes in apps   | Logs collected from stdout by filelog receiver |
| Single ingestion pipeline | Collector handles logs+metrics+traces          |
| Dynamic configuration     | Registry + ConfigBuilder + AgentManager        |
| Safe rollouts             | Canary reload + rollback activity              |
| Horizontal scaling        | Distributed Loki + S3/MinIO chunk backend      |
| AI-ready later            | Export pipelines in TREE F, P3                 |

---

# âœ… TL;DR (One Sentence)

> **Logs now, metrics later, traces later â€” all via one collector â€” automatically discovered, dynamically configured, hot-reloaded, monitored, and ready for AI training export.**

---

## ğŸ¯ NEXT (Your immediate engineering move)

Pick one and I will generate it **ready to paste into your code**:

| Option  | I will deliver                                                 |
| ------- | -------------------------------------------------------------- |
| **A**   | `otel-collector.yaml` (filelogâ†’Loki, production safe)          |
| **B**   | `collector_dynamic_reload_workflow.py` (Temporal workflow)     |
| **C**   | Full `control_plane/` repo skeleton (classes + activity stubs) |
| **D**   | Kubernetes DaemonSet version of the collector                  |
| **ALL** | Everything, ordered in correct implementation sequence         |

Reply with: **A**, **B**, **C**, **D**, or **ALL**.
Great question â€” this is **core architecture clarity**.
Once you understand **Control Plane vs Data Plane**, everything else becomes obvious.

---

# âœ… SHORT DEFINITION

| Plane             | What it Does                                                                  | Example Components                                                     | Your Responsibility                         |
| ----------------- | ----------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------- |
| **Data Plane**    | Actually **collects, processes, and ships logs/metrics/traces**               | OpenTelemetry Collector, Loki, Prometheus, Tempo, Jaeger               | Reliable + fast + stable ingestion          |
| **Control Plane** | **Decides configuration**, manages discovery, orchestration, scaling, reloads | DiscoveryService, Registry, ConfigBuilder, AgentManager, HealthMonitor | Intelligence + automation + dynamic updates |

**Data Plane = â€œDo the work.â€**
**Control Plane = â€œDecide *how* the work happens.â€**

---

# âœ… MASTER SYSTEM TREE (Control Plane vs Data Plane)

```
START: OBSERVABILITY PLATFORM ARCHITECTURE
â”‚
? layer = [DATA_PLANE | CONTROL_PLANE | BACKENDS]
â”‚
â”œâ”€ DATA_PLANE     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE A (Log/Metrics/Traces flow)
â”œâ”€ CONTROL_PLANE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE B (Automation & Management)
â””â”€ BACKENDS       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TREE C (Storage & Query)
```

---

## TREE A â€” DATA PLANE (the actual runtime pipeline)

```
DATA_PLANE
â”‚
? signal = [logs | metrics | traces]
â”‚
â”œâ”€ logs  (current P0)
â”‚   USE:
â”‚     â€¢ OTelCollectorAgent
â”‚         - filelog_receiver()
â”‚         - attributes_processor()
â”‚         - resource_processor()
â”‚         - batch_processor()
â”‚         - loki_exporter()
â”‚   PURPOSE:
â”‚     Move logs â†’ from stdout â†’ to Loki reliably.

â”œâ”€ metrics (P1 later)
â”‚   USE:
â”‚     â€¢ OTelCollectorAgent (same agent!)
â”‚         - otlp_receiver()
â”‚         - metric_aggregation_processor()
â”‚         - prometheusremotewrite_exporter()
â”‚   PURPOSE:
â”‚     Collect app metrics â†’ Prometheus â†’ dashboards â†’ alerts.

â””â”€ traces (P1/P2 later)
    USE:
      â€¢ OTelCollectorAgent (same agent!)
          - otlp_receiver()
          - sampling_processor()
          - tempo_exporter() or jaeger_exporter()
    PURPOSE:
      Distributed tracing â†’ root cause analysis.

```

### DATA PLANE Key Rule

**One collector â†’ Three pipelines**
You do **NOT** deploy different agents later.

---

## TREE B â€” CONTROL PLANE (automation & intelligence layer)

```
CONTROL_PLANE
â”‚
? function = [discover | store | build_config | deploy_config | verify | heal]
â”‚
â”œâ”€ discover  (detect new services/log paths)
â”‚   CLASS: DiscoveryService
â”‚   METHODS:
â”‚     - watch_docker_events()
â”‚     - watch_k8s_api()
â”‚     - detect_container_log_paths()

â”œâ”€ store  (state tracking)
â”‚   CLASS: LogSourceRegistry
â”‚   METHODS:
â”‚     - register_log_target()
â”‚     - update_source_labels()
â”‚     - list_sources()

â”œâ”€ build_config  (collector config generation)
â”‚   CLASS: OtelConfigBuilder
â”‚   METHODS:
â”‚     - generate_log_pipeline()
â”‚     - generate_metrics_pipeline()     (later)
â”‚     - generate_traces_pipeline()      (later)
â”‚     - validate_config()

â”œâ”€ deploy_config  (reload collector safely)
â”‚   CLASS: AgentManager
â”‚   METHODS:
â”‚     - push_config_to_collector()
â”‚     - canary_reload()
â”‚     - rollback_config()
â”‚     - verify_reload()

â”œâ”€ verify (health & correctness)
â”‚   CLASS: HealthMonitor
â”‚   METHODS:
â”‚     - check_collector_queue_backpressure()
â”‚     - check_loki_push_errors()
â”‚     - check_end_to_end_sample_log()

â””â”€ heal (self-recovery control loops)
    CLASS: RepairController (optional P2)
    METHODS:
      - auto_apply_fallback_config()
      - restart_failed_components()
      - notify_on_persistent_failure()

```

### CONTROL PLANE Key Rule

**Control Plane never touches logs.
It only decides how logs should be collected.**

---

## TREE C â€” BACKENDS (storage & query layer)

```
BACKENDS
â”‚
â”œâ”€ Logs Store â†’ Loki
â”‚   - chunk_storage (S3 / MinIO)
â”‚   - index_store (boltdb-shipper)
â”‚   - query_frontend (parallelized search)

â”œâ”€ Metrics Store â†’ Prometheus / Mimir
â”‚   - time-series retention
â”‚   - alert evaluation

â””â”€ Traces Store â†’ Tempo / Jaeger
    - span graph storage
    - service dependency maps
```

---

# âœ… FLOW TOGETHER (COMBINED SYSTEM SEQUENCE, PRODUCTION)

```
[APP] â”€â”€ stdout â”€â”€â–¶ [DATA PLANE: OTel Collector] â”€â”€â–¶ [LOKI]

[New container starts] â”€â”€â–¶ [CONTROL PLANE: Discovery] 
    â””â”€â–¶ Registry
        â””â”€â–¶ ConfigBuilder
            â””â”€â–¶ AgentManager reloads collector
                â””â”€â–¶ HealthMonitor verifies success
```

Data Plane **does the work**.
Control Plane **keeps it correct, updated, safe, auto-healing**.

---
