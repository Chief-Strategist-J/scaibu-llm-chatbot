# **Temporal Orchestrator â€” README**

## 1. Overview

This directory contains a **Temporal.io orchestration system** for managing containerized services in the LLM chatbot infrastructure.

**Components:**

* **Activities** â€“ Individual operations (e.g., container start/stop, monitoring).
* **Workflows** â€“ Orchestration logic to coordinate activities.
* **Workers** â€“ Poll task queues and execute workflows/activities.
* **Temporal Server** â€“ Manages state and workflow durability.
* **Trigger / Stop scripts** â€“ Start or terminate workflows, organized per service.

---

## 2. Folder Structure

```
temporal-orchestrator/
â”‚
â”œâ”€â”€ activities/
â”‚   â”œâ”€â”€ ai_proxy_container_activity.py
â”‚   â”œâ”€â”€ my_new_activity.py           # Example new activity
â”‚   â””â”€â”€ common_activity/
â”‚       â”œâ”€â”€ start_grafana_activity.py
â”‚       â”œâ”€â”€ configure_grafana_activity.py
â”‚       â”œâ”€â”€ loki_activity.py
â”‚       â”œâ”€â”€ otel_activity.py
â”‚       â”œâ”€â”€ promotheus_activity.py
â”‚       â””â”€â”€ promtail_activity.py
â”‚
â”œâ”€â”€ dynamicconfig/
â”‚   â””â”€â”€ development-sql.yaml
â”‚
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ logging_pipeline_workflow.py
â”‚   â””â”€â”€ my_new_workflow.py           # Example new workflow
â”‚
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ logging_pipeline_worker.py
â”‚   â””â”€â”€ my_new_worker.py             # Example new worker
â”‚
â”œâ”€â”€ trigger/
â”‚   â””â”€â”€ ai_proxy_container/          # Example service folder
â”‚       â”œâ”€â”€ start.py
â”‚       â””â”€â”€ stop.py
â”‚   â””â”€â”€ knowledge_graph/             # Example service folder
â”‚       â”œâ”€â”€ start.py
â”‚       â””â”€â”€ stop.py
â”‚
â”œâ”€â”€ temporal-orchestrator-compose.yaml
â””â”€â”€ README.md
```

> **Note:** Each new service gets its own folder under `trigger/` with its `start.py` and `stop.py` scripts.

---

## 3. Adding a New Activity

**Path:** `activities/my_new_activity.py`

```python
from temporalio import activity
import asyncio

@activity.defn
async def my_new_activity(arg: str) -> str:
    """Minimal new activity"""
    await asyncio.sleep(0.1)
    return f"done:{arg}"
```

---

## 4. Adding a New Workflow

**Path:** `workflows/my_new_workflow.py`

```python
from datetime import timedelta
from temporalio import workflow
from temporalio.common import RetryPolicy

@workflow.defn
class MyNewWorkflow:
    @workflow.run
    async def run(self, param: str) -> str:
        from activities.my_new_activity import my_new_activity

        result = await workflow.execute_activity(
            my_new_activity,
            param,
            start_to_close_timeout=timedelta(minutes=2),
            retry_policy=RetryPolicy(
                maximum_attempts=3,
                initial_interval=timedelta(seconds=10),
            ),
        )
        return result
```

---

## 5. Adding a New Worker (Independent)

**Path:** `workers/my_new_worker.py`

```python
import asyncio
from temporalio.client import Client
from temporalio.worker import Worker
from workflows.my_new_workflow import MyNewWorkflow
from activities.my_new_activity import my_new_activity

async def main():
    client = await Client.connect("localhost:7233")
    worker = Worker(
        client,
        task_queue="my-new-task-queue",
        workflows=[MyNewWorkflow],
        activities=[my_new_activity]
    )
    await worker.run()

if __name__ == "__main__":
    asyncio.run(main())
```

> No edits to existing workers are required.

---

## 6. Trigger Script for a Service

**Path:** `trigger/knowledge_graph/start.py`

```python
import asyncio
from temporalio.client import Client
from workflows.my_new_workflow import MyNewWorkflow

async def main():
    client = await Client.connect("localhost:7233")
    result = await client.execute_workflow(
        MyNewWorkflow.run,
        "input-value",
        id="knowledge_graph_1",
        task_queue="my-new-task-queue",
    )
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 7. Stop Script for a Service

**Path:** `trigger/knowledge_graph/stop.py`

```python
import asyncio
from temporalio.client import Client

async def main():
    client = await Client.connect("localhost:7233")
    await client.terminate_workflow("knowledge_graph_1", reason="manual stop")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 8. Commands

### Start Temporal Infrastructure

```bash
cd infrastructure/orchestrator

docker-compose -f temporal-orchestrator-compose.yaml down -v

docker system prune -f

docker-compose -f temporal-orchestrator-compose.yaml up -d
cd ../..

source /home/j/live/dinesh/llm-chatbot-python/.venv/bin/activate

```

### Verify Services

```bash
docker ps
curl http://0.0.0.0:8080/namespaces/default/workflows
docker exec temporal-postgresql psql -U temporal -d temporal -c "SELECT 1;"
```

### Start Your Worker

```bash
cd workers
python3 my_new_worker.py

// start workers

source /home/j/live/dinesh/llm-chatbot-python/.venv/bin/activate


python infrastructure/orchestrator/workers/logs_pipeline_worker.py 

python infrastructure/orchestrator/workers/database_pipeline_worker.py 

python infrastructure/orchestrator/workers/metrics_pipeline_worker.py 

python infrastructure/orchestrator/workers/tracing_pipeline_worker.py 

python service/llm_chat_app/worker/workers/chat_worker.py 

python infrastructure/observability/workers/logs_pipeline_worker.py



```

### Trigger Service Workflow

```bash
cd trigger/knowledge_graph
python3 start.py

// Trigger Service Workflow

python infrastructure/orchestrator/trigger/common/logs_pipeline_start.py 

python infrastructure/orchestrator/trigger/common/database_pipeline_start.py 

python infrastructure/orchestrator/trigger/common/metrics_pipeline_start.py 

python infrastructure/orchestrator/trigger/common/tracing_pipeline_start.py 

python infrastructure/orchestrator/trigger/common/run_logs_pipeline_then_stdout_ingest.py

python infrastructure/orchestrator/trigger/common/start_application_stdout_ingest.py

python service/llm_chat_app/worker/workflows/chat_setup_workflow.py

python service/llm_chat_app/worker/workflows/chat_cleanup_workflow.py 




```

### Logs Pipeline Workflow

```bash
START
 â”‚
 â”‚â”€â”€ Receive `params`
 â”‚
 â”‚â”€â”€ Configure Retry Policy
 â”‚      â€¢ initial_interval = 1s
 â”‚      â€¢ maximum_interval = 10s
 â”‚      â€¢ maximum_attempts = 3
 â”‚
 â”‚â”€â”€ Set Activity Timeout
 â”‚      â€¢ start_to_close_timeout = 5 minutes
 â”‚
 â”‚
 â”œâ”€â”€ Step 1: Stop OpenTelemetry Collector
 â”‚        Activity: stop_opentelemetry_collector(params)
 â”‚        Behavior:
 â”‚           - Retries based on RetryPolicy
 â”‚           - Fails workflow if all retry attempts fail
 â”‚
 â”œâ”€â”€ Step 2: Delete OpenTelemetry Collector
 â”‚        Activity: delete_opentelemetry_collector(params)
 â”‚        Behavior:
 â”‚           - Same retry/timeout controls
 â”‚
 â”œâ”€â”€ Step 3: Start OpenTelemetry Collector
 â”‚        Activity: start_opentelemetry_collector(params)
 â”‚        Behavior:
 â”‚           - Ensures collector is active again for pipeline
 â”‚
 â”œâ”€â”€ Step 4: Start Loki Service
 â”‚        Activity: start_loki_activity(params)
 â”‚        Behavior:
 â”‚           - Initiates Loki to ingest logs
 â”‚
 â”œâ”€â”€ Step 5: Start Grafana Service
 â”‚        Activity: start_grafana_activity(params)
 â”‚        Behavior:
 â”‚           - Ensures Grafana is up to visualize the logs
 â”‚
 â”‚
 â””â”€â”€ END â†’ Return message:
         "Logs pipeline fully configured"

```


### Database Pipeline Workflow

```bash
START
  â”‚
  â–¼
Receive `service_name`
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Start Neo4j Container
  â”‚     Activity: start_neo4j_container(service_name)
  â”‚     Behavior:
  â”‚       - start_to_close_timeout = 5 minutes
  â”‚       - retry_policy: maximum_attempts = 3
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Start Qdrant Container
  â”‚     Activity: start_qdrant_container(service_name)
  â”‚     Behavior:
  â”‚       - start_to_close_timeout = 5 minutes
  â”‚       - retry_policy: maximum_attempts = 3
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "Database pipeline fully configured: Neo4j + Qdrant"

```

### Metrics Pipeline Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Validate params
  â”‚      â€¢ Must be dict
  â”‚      â€¢ Must contain service_name (string)
  â”‚
  â”œâ”€â”€ If invalid â†’ END â†’ "Error: Invalid params provided"
  â”œâ”€â”€ If missing/invalid service_name â†’ END â†’ "Error: service_name is required and must be string"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 1s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Activity Timeout
  â”‚      â€¢ start_to_close_timeout = 5 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Start Prometheus Container
  â”‚     Activity: start_prometheus_container(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Returns truthy if successful
  â”‚
  â”œâ”€â”€ If result is falsy â†’ END â†’ "Error: Failed to start Prometheus container"
  â”‚
  â–¼
Sleep 2 seconds
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Start Grafana Container
  â”‚     Activity: start_grafana_container(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Returns truthy if successful
  â”‚
  â”œâ”€â”€ If result is falsy â†’ END â†’ "Error: Failed to start Grafana container"
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "Metrics pipeline fully configured: Prometheus + Grafana + Dashboard"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXCEPTION HANDLING
  â”‚
  â””â”€â”€ If any exception occurs:
          END â†’ "Error: Metrics pipeline failed: <error>"


```


### Tracing Pipeline Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Validate params
  â”‚      â€¢ Must be dict
  â”‚      â€¢ Must contain service_name (string)
  â”‚
  â”œâ”€â”€ If invalid â†’ END â†’ "Error: Invalid params provided"
  â”œâ”€â”€ If missing/invalid service_name â†’ END â†’ "Error: service_name is required and must be string"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 1s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Activity Timeout
  â”‚      â€¢ start_to_close_timeout = 5 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Start Jaeger Container
  â”‚     Activity: start_jaeger_container(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Returns truthy if successful
  â”‚
  â”œâ”€â”€ If result is falsy â†’ END â†’ "Error: Failed to start Jaeger container"
  â”‚
  â–¼
Sleep 2 seconds
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Start Grafana Container
  â”‚     Activity: start_grafana_container(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Returns truthy if successful
  â”‚
  â”œâ”€â”€ If result is falsy â†’ END â†’ "Error: Failed to start Grafana container"
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "Tracing pipeline fully configured: Jaeger + Grafana + Dashboard"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXCEPTION HANDLING
  â”‚
  â””â”€â”€ If any exception occurs:
          END â†’ "Error: Tracing pipeline failed: <error>"


```


### Application Stdout Ingest Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeouts
  â”‚      â€¢ t = 5 minutes
  â”‚      â€¢ some steps = 30 seconds
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Configure Application Stdout Pipeline
  â”‚     Activity: logs_configure_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 5-minute timeout
  â”‚       - Returns configuration object
  â”‚
  â”œâ”€â”€ If conf is falsy â†’ END â†’ "configuration failed"
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Add Loki Datasource
  â”‚     Activity: add_loki_datasource_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 30-second timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sleep 5 seconds
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Discover Local Log Files
  â”‚     Activity: discover_log_files_activity(conf)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 5-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Discover Docker Log Files
  â”‚     Activity: discover_docker_logs_activity(conf)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 5-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Merge Discoveries
  â”‚      â€¢ local_logs + docker_logs
  â”‚      â€¢ Remove duplicates
  â”‚      â€¢ Sort list
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Enrich Logs With Labels
  â”‚     Activity: label_enrichment_activity(
  â”‚         {"files": discovered, "labels": conf.get("labels", {})}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 5-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Build Shipping Parameters
  â”‚      â€¢ files = enriched[].path
  â”‚      â€¢ labels = enriched[0].labels OR conf.labels
  â”‚      â€¢ protocol = otlp
  â”‚      â€¢ endpoint = http://localhost:4318
  â”‚      â€¢ batch_size
  â”‚      â€¢ flush_interval_seconds
  â”‚      â€¢ timeout_seconds = 10
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 6: Tail and Ship Logs
  â”‚     Activity: tail_and_ship_logs_activity(ship_params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 30-second timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "ingest started: discovered={len(discovered)} enriched={len(enriched)} status={result.status}"

```



### Container Logs Ingest Workflow

```bash
START
  â”‚
  â–¼
Receive inputs
  â”‚      â€¢ template_path: str
  â”‚      â€¢ log_paths: list
  â”‚      â€¢ service_name: str
  â”‚      â€¢ loki_endpoint: str
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 1s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 5 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Generate and Validate Config
  â”‚     Activity: generate_and_validate_config_activity(
  â”‚         template_path,
  â”‚         log_paths,
  â”‚         service_name,
  â”‚         loki_endpoint
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Returns config_str
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Push and Reload Configuration
  â”‚     Activity: push_and_reload_activity(config_str)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "container_logs ingest configured"


```

### OTLP from Apps Ingest Workflow

```bash
START
  â”‚
  â–¼
Receive inputs
  â”‚      â€¢ template_path: str
  â”‚      â€¢ service_name: str
  â”‚      â€¢ otlp_endpoint: str
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 1s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 5 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Enable OTLP Receiver
  â”‚     Activity: enable_otlp_receiver_activity(
  â”‚         template_path,
  â”‚         service_name,
  â”‚         otlp_endpoint
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â”‚       - Produces config_path
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Collect and Route OTLP Traffic
  â”‚     Activity: collect_and_route_otlp_activity(config_path)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
END â†’ "otlp_from_apps ingest enabled"
```


### Chat Setup Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Clone params â†’ p
  â”‚      â€¢ p["context"] = "/home/j/live/dinesh/llm-chatbot-python/service/llm_chat_app"
  â”‚
  â–¼
Log: "ChatSetupWorkflow start"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 2s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 10 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Start Neo4j Dependency
  â”‚     Activity: start_neo4j_dependency_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Verify Cloudflare Dependency
  â”‚     Activity: verify_cloudflare_dependency_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Build Chat Image
  â”‚     Activity: build_chat_image_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Run Chat Container
  â”‚     Activity: run_chat_container_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Check Chat Health
  â”‚     Activity: check_chat_health_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log: "ChatSetupWorkflow complete"
  â”‚
  â–¼
END â†’ "chat_setup_complete"

```


### Chat Cleanup Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Log: "ChatCleanupWorkflow start"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 2s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 10 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Stop Chat Container
  â”‚     Activity: stop_chat_container_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Delete Chat Container
  â”‚     Activity: delete_chat_container_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Delete Chat Image
  â”‚     Activity: delete_chat_image_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Stop Neo4j Dependency
  â”‚     Activity: stop_neo4j_dependency_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Delete Neo4j Dependency
  â”‚     Activity: delete_neo4j_dependency_activity(params)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 10-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log: "ChatCleanupWorkflow complete"
  â”‚
  â–¼
END â†’ "chat_cleanup_complete"


```


### Flyio Deployment Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Clone params â†’ p
  â”‚      â€¢ p.setdefault("service_name", "flyio-deploy")
  â”‚
  â–¼
Log: "FlyioDeploymentWorkflow start"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 2s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 15 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Generate Deployment Configs
  â”‚     Activity: generate_deployment_configs_activity(
  â”‚         {**p, "platforms": ["flyio"]}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Create Fly.io App
  â”‚     Activity: create_flyio_app_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Set Fly.io Secrets
  â”‚     Activity: set_flyio_secrets_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Deploy to Fly.io
  â”‚     Activity: deploy_to_flyio_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â”‚       - Produces deploy_result dict (possibly)
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Determine Deployment URL
  â”‚      â€¢ If deploy_result contains "deployment_url" â†’ use it
  â”‚      â€¢ Else fallback: https://{app_name}.fly.dev
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Check Deployment Health
  â”‚     Condition: deployment_url is truthy
  â”‚     Activity: check_deployment_health_activity(
  â”‚         {**p, "url": deployment_url}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log: "FlyioDeploymentWorkflow complete"
  â”‚
  â–¼
END â†’ "flyio_deploy_complete"


```


### Railway Deployment Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Clone params â†’ p
  â”‚      â€¢ p.setdefault("service_name", "railway-deploy")
  â”‚
  â–¼
Log: "RailwayDeploymentWorkflow start"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 2s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 15 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Generate Deployment Configs
  â”‚     Activity: generate_deployment_configs_activity(
  â”‚         {**p, "platforms": ["railway"]}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Create Railway Project
  â”‚     Activity: create_railway_project_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Set Railway Environment Variables
  â”‚     Activity: set_railway_env_vars_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Deploy to Railway
  â”‚     Activity: deploy_to_railway_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â”‚       - Produces deploy_result dict (maybe)
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Determine Deployment URL
  â”‚      â€¢ If deploy_result is dict â†’ deployment_url = deploy_result["deployment_url"]
  â”‚      â€¢ Else â†’ deployment_url = None
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Check Deployment Health
  â”‚     Condition: deployment_url is truthy
  â”‚     Activity: check_deployment_health_activity(
  â”‚         {**p, "url": deployment_url}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log: "RailwayDeploymentWorkflow complete"
  â”‚
  â–¼
END â†’ "railway_deploy_complete"


```


### Render Deployment Workflow

```bash
START
  â”‚
  â–¼
Receive `params`
  â”‚
  â–¼
Clone params â†’ p
  â”‚      â€¢ p.setdefault("service_name", "render-deploy")
  â”‚
  â–¼
Log: "RenderDeploymentWorkflow start"
  â”‚
  â–¼
Configure Retry Policy
  â”‚      â€¢ initial_interval = 2s
  â”‚      â€¢ maximum_interval = 10s
  â”‚      â€¢ maximum_attempts = 3
  â”‚
  â–¼
Set Timeout
  â”‚      â€¢ start_to_close_timeout = 15 minutes
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 1: Generate Deployment Configs
  â”‚     Activity: generate_deployment_configs_activity(
  â”‚         {**p, "platforms": ["render"]}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 2: Create Render Blueprint
  â”‚     Activity: create_render_blueprint_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 3: Push Code to GitHub
  â”‚     Activity: push_to_github_activity(p)
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â”‚       - Produces push_result dict (maybe)
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 4: Deploy to Render
  â”‚     Activity: deploy_to_render_activity(
  â”‚         {**p, **(push_result or {})}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Determine Deployment URL
  â”‚      â€¢ If push_result contains deployment_url â†’ use it
  â”‚      â€¢ Else fallback to push_result.repo_url
  â”‚
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step 5: Check Deployment Health
  â”‚     Condition: deployment_url is truthy
  â”‚     Activity: check_deployment_health_activity(
  â”‚         {**p, "url": deployment_url}
  â”‚     )
  â”‚     Behavior:
  â”‚       - Uses retry_policy
  â”‚       - Uses 15-minute timeout
  â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log: "workflow RenderDeploymentWorkflow complete"
  â”‚
  â–¼
END â†’ "render_deploy_complete"



```


### Stop Infrastructure

```bash
docker-compose -f temporal-orchestrator-compose.yaml down
```

---

## 9. Visual Pipeline (for Knowledge Graph Service)

```
[Trigger Script]         trigger/knowledge_graph/start.py
       â”‚
       â–¼
[Temporal Server]         localhost:7233
       â”‚
       â–¼
[Worker]                  workers/my_new_worker.py
       â”‚
       â–¼
[Workflow Execution]      workflows/my_new_workflow.py
       â”‚
       â””â”€â–º calls activities/my_new_activity.py
       â”‚
       â–¼
[Results / State]         Temporal DB (Postgres)
       â”‚
       â–¼
[Web UI / Logs]           http://localhost:8080
```

# ğŸŒ² **MASTER EXECUTION + RELATIONSHIP TREE**

```
PIPELINE_SYSTEM
â”‚
â”œâ”€ CONTROL_PLANE  (META: decisions, identity, routing, meaning)
â”‚   â”‚
â”‚   â”œâ”€ WorkflowConfig  (meta config)
â”‚   â”‚     â”‚
â”‚   â”‚     â”œâ”€ service_name        = logical pipeline identity
â”‚   â”‚     â”œâ”€ workflow_name       = workflow class to start
â”‚   â”‚     â”œâ”€ task_queue          = execution route (auto: <service_name>-queue)
â”‚   â”‚     â”œâ”€ temporal_host       = cluster connection target
â”‚   â”‚     â””â”€ web_ui_url          = observation URL (optional / meta only)
â”‚   â”‚
â”‚   â”œâ”€ PipelineExecutor  (meta orchestrator)
â”‚   â”‚     â”‚
â”‚   â”‚     â”œâ”€ reads WorkflowConfig
â”‚   â”‚     â”œâ”€ connects to temporal_host
â”‚   â”‚     â””â”€ start_workflow(
â”‚   â”‚           workflow_name,
â”‚   â”‚           arg = service_name,
â”‚   â”‚           task_queue = task_queue
â”‚   â”‚        )
â”‚   â”‚
â”‚   â””â”€ Workflow State Machine (meta logic)
â”‚         â”‚
â”‚         â””â”€ Describes *order* of operations (not actual work):
â”‚                start_opentelemetry â†’ start_loki â†’ start_grafana â†’ return result
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ TRAVEL ACROSS NETWORK
                            (Temporal API call)
```

---

# ğŸ›°ï¸ **TEMPORAL ROUTING LAYER (Control â†’ Execution Bridge)**

```
TEMPORAL_SERVER
â”‚
â”œâ”€ Receives workflow start request
â”‚
â”œâ”€ Creates Workflow Execution History (META: deterministic timeline)
â”‚
â””â”€ SCHEDULE_WORKFLOW_TASK
       â”‚
       â””â”€ Route by queue:
           task_queue = "<service_name>-queue"
```

**This is the critical relationship:**

```
WorkflowConfig.task_queue  MUST MATCH  WorkerConfig.task_queue
```

This is the **binding point** of the system.

---

# âš™ï¸ **DATA_PLANE (Execution Happens Here)**

```
DATA_PLANE
â”‚
â”œâ”€ WorkerConfig  (meta execution environment description)
â”‚   â”‚
â”‚   â”œâ”€ host           = temporal endpoint
â”‚   â”œâ”€ task_queue     = execution queue (same as WorkflowConfig)
â”‚   â”œâ”€ namespace      = logical tenant
â”‚   â””â”€ max_concurrency  (optional runtime tuning)
â”‚
â”œâ”€ Worker (runtime executor process)
â”‚   â”‚
â”‚   â”œâ”€ Registers:
â”‚   â”‚     - Workflows: [LogsPipelineWorkflow]
â”‚   â”‚     - Activities: [start_loki, start_grafana, etc]
â”‚   â”‚
â”‚   â””â”€ Listens on task_queue
â”‚         â”‚
â”‚         â””â”€ When workflow tasks arrive â†’ run workflow logic step-by-step
â”‚
â””â”€ Activity Executor (real work happens here)
      â”‚
      â”œâ”€ start_opentelemetry_collector(service_name)
      â”œâ”€ start_loki_activity(service_name)
      â”œâ”€ start_grafana_activity(service_name)
      â””â”€ etc...
      â”‚
      â””â”€ These functions produce **real side-effects**:
            - Launch containers
            - Configure services
            - Apply setup changes
```

---

# ğŸ›ï¸ **WORKFLOW EXECUTION PLAY-BY-PLAY**

```
Workflow (META: high-level sequence)
â”‚
â””â”€ Step 1: schedule activity: start_opentelemetry_collector
      â”‚
      â””â”€ Temporal routes â†’ Worker â†’ Activity executes (real work)
             â”‚
             â””â”€ Return OK â†’ Workflow proceeds

â””â”€ Step 2: schedule activity: start_loki_activity
      â”‚
      (same dispatch-execute-return pattern)

â””â”€ Step 3: schedule activity: start_grafana_activity

â””â”€ Workflow returns: "Logs pipeline fully configured"
```

---

# âš¡ RELATIONSHIP CLASSIFICATION (Final clarity)

| Relationship                                        | Direction    | Type              | Explanation                                   |
| --------------------------------------------------- | ------------ | ----------------- | --------------------------------------------- |
| PipelineExecutor â†’ WorkflowConfig                   | uses         | META              | Executor reads config to know *what to start* |
| PipelineExecutor â†’ Temporal Server                  | commands     | CONTROL           | Executor tells Temporal to create workflow    |
| WorkflowConfig.task_queue â†” WorkerConfig.task_queue | binding link | ROUTING           | Ensures workflow tasks and worker match       |
| Temporal Server â†’ Worker                            | dispatches   | EXECUTION ROUTING | Server delivers tasks to worker queue         |
| Worker â†’ Workflow                                   | hosts        | EXECUTION CONTEXT | Worker runs workflow state machine            |
| Workflow â†’ Activities                               | delegates    | TASK EXECUTION    | Workflow requests work, activities do work    |
| Activities â†’ External Systems                       | acts         | REAL EFFECT       | System state changes happen here              |

---
