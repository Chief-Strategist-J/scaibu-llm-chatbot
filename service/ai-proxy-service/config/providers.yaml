llm:
  default: cloudflare
  
  cloudflare:
    enabled: true
    model: "@cf/meta/llama-3.2-1b-instruct"
    max_tokens: 256
    temperature: 0.7
    
  openai:
    enabled: false
    model: "gpt-3.5-turbo"
    max_tokens: 256
    temperature: 0.7
    
  grok:
    enabled: false
    model: "grok-beta"
    max_tokens: 256
    temperature: 0.7
    base_url: "https://api.x.ai/v1"
    
  anthropic:
    enabled: false
    model: "claude-3-haiku-20240307"
    max_tokens: 256
    temperature: 0.7
