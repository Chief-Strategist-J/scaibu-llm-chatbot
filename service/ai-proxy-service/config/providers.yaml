---
llm:
  default: cloudflare

  cloudflare:
    enabled: true
    model: "@cf/meta/llama-3.2-1b-instruct"
    max_tokens: 256
    temperature: 0.7

  openai:
    enabled: false
    model: "gpt-3.5-turbo"
    max_tokens: 256
    temperature: 0.7

  grok:
    enabled: false
    model: "grok-beta"
    max_tokens: 256
    temperature: 0.7
    base_url: "https://api.x.ai/v1"

  anthropic:
    enabled: false
    model: "claude-3-haiku-20240307"
    max_tokens: 256
    temperature: 0.7
