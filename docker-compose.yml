---
name: ${COMPOSE_PROJECT_NAME:-llm-stack-8gb}

networks:
  llm-network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-172.25.0.0/16}

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-llm
    hostname: ollama
    restart: unless-stopped

    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-1}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-1}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-2048}
      - OLLAMA_NUM_THREADS=${OLLAMA_NUM_THREADS:-4}
      - OLLAMA_FLASH_ATTENTION=false
      - OLLAMA_DEBUG=false

    ports:
      - "${OLLAMA_PORT:-11434}:11434"

    volumes:
      - ollama_data:/root/.ollama

    networks:
      - llm-network

    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4g}
          cpus: "${OLLAMA_CPUS:-2.0}"
        reservations:
          memory: 2g
          cpus: "1.0"

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

volumes:
  ollama_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/ollama
